<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/infinite-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/infinite-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"awayx.online","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"livere","storage":true,"lazyload":false,"nav":null,"activeClass":"livere"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Abstract在本文中，我们对凝视点预测（fixation prediction）和显著性对象分割（salient object segmentation）算法进行了广泛的评估，并对主要数据集的统计数据进行了分析。我们的分析指出了现有显著性对象基准中的严重设计缺陷，称之为数据集设计偏差，这种偏差过度强调了显著性的刻板概念。数据集设计偏差不仅导致凝视点与显著性对象分割之间的脱节，还误导了算法的设计">
<meta property="og:type" content="article">
<meta property="og:title" content="The Secrets of Salient Object Segmentation">
<meta property="og:url" content="http://awayx.online/2024/08/12/The%20Secrets%20of%20Salient%20Object%20Segmentation/index.html">
<meta property="og:site_name" content="AwaySpace">
<meta property="og:description" content="Abstract在本文中，我们对凝视点预测（fixation prediction）和显著性对象分割（salient object segmentation）算法进行了广泛的评估，并对主要数据集的统计数据进行了分析。我们的分析指出了现有显著性对象基准中的严重设计缺陷，称之为数据集设计偏差，这种偏差过度强调了显著性的刻板概念。数据集设计偏差不仅导致凝视点与显著性对象分割之间的脱节，还误导了算法的设计">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://vip.helloimg.com/i/2024/08/12/66b9fc336035f.png">
<meta property="og:image" content="https://vip.helloimg.com/i/2024/08/12/66b9fcf9933aa.png">
<meta property="og:image" content="https://vip.helloimg.com/i/2024/08/12/66b9fd4868326.png">
<meta property="article:published_time" content="2024-08-12T13:21:54.000Z">
<meta property="article:modified_time" content="2024-08-18T12:53:44.938Z">
<meta property="article:author" content="AwayX">
<meta property="article:tag" content="论文笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://vip.helloimg.com/i/2024/08/12/66b9fc336035f.png">

<link rel="canonical" href="http://awayx.online/2024/08/12/The%20Secrets%20of%20Salient%20Object%20Segmentation/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>The Secrets of Salient Object Segmentation | AwaySpace</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">AwaySpace</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">5</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">42</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">32</span></a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://awayx.online/2024/08/12/The%20Secrets%20of%20Salient%20Object%20Segmentation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/TheRightToCry.jpg">
      <meta itemprop="name" content="AwayX">
      <meta itemprop="description" content="嘿嘿">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AwaySpace">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          The Secrets of Salient Object Segmentation
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2024-08-12 21:21:54" itemprop="dateCreated datePublished" datetime="2024-08-12T21:21:54+08:00">2024-08-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2024-08-18 20:53:44" itemprop="dateModified" datetime="2024-08-18T20:53:44+08:00">2024-08-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文笔记</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Computer-Vision/" itemprop="url" rel="index"><span itemprop="name">Computer Vision</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Computer-Vision/Salient-Object-Ranking/" itemprop="url" rel="index"><span itemprop="name">Salient Object Ranking</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Computer-Vision/Salient-Object-Ranking/image/" itemprop="url" rel="index"><span itemprop="name">image</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Computer-Vision/Salient-Object-Ranking/image/2014/" itemprop="url" rel="index"><span itemprop="name">2014</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>在本文中，我们对凝视点预测（fixation prediction）和显著性对象分割（salient object segmentation）算法进行了广泛的评估，并对主要数据集的统计数据进行了分析。我们的分析指出了现有显著性对象基准中的严重设计缺陷，称之为<strong>数据集设计偏差</strong>，这种偏差<strong>过度强调了显著性的刻板概念</strong>。数据集设计偏差不仅导致凝视点与显著性对象分割之间的脱节，还误导了算法的设计。基于我们的分析，我们提出了一个新的高质量数据集，该数据集<strong>同时提供了凝视点和显著性对象分割的真实标注</strong>。通过同时展示凝视点和显著性对象，我们能够弥合凝视点和显著性对象之间的差距，并提出了一种新的显著性对象分割方法。最后，我们在三个现有的数据集上报告了显著性对象分割的显著基准进展。</p>
<span id="more"></span>

<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>自下而上的视觉显著性指的是选择重要视觉信息以便进一步处理的能力。这一机制不仅对人类有用，对计算机视觉也同样有益。与目标检测&#x2F;识别等其他主题不同，显著性并不是一个定义明确的术语。在计算机视觉中，大多数工作集中在以下两个具体的显著性任务之一：<strong>凝视点预测和显著性对象分割</strong>。在凝视实验中，显著性通过眼睛凝视来表达。实验对象被要求观看每张图像几秒钟，同时记录他们的眼睛凝视点。算法的目标是计算图像的概率图，以预测实际的人类眼睛凝视模式。另一种方式是在显著性对象分割数据集中，图像标注者通过为图像中被认为是显著的对象绘制像素级准确的轮廓来进行标注。然后算法需要生成一个与标注的显著对象掩码相匹配的图像图。</p>
<p>各种凝视点和显著性对象分割的数据集为分析算法提供了客观的方式。然而，现有的方法存在两个主要限制：1）专注于一种显著性的算法<strong>往往忽视了与另一种显著性的关联</strong>。2）主要基于一个数据集的基准测试<strong>往往会导致过拟合该数据集固有的偏差</strong>。在本文中，我们通过为<strong>PASCAL 2010</strong> [12]数据集中的850张现有图像添加眼睛凝视和显著性对象分割标注，探讨了凝视点预测和显著性对象分割之间的关联。在第3节中，我们讨论了通过使图像采集和图像标注彼此独立，可以避免数据集设计偏差——这种偏差是由实验者对数据集图像的不自然选择所引起的特定类型的偏差。通过在同一组图像中同时展示凝视点和显著性对象标签，我们报告了一系列有趣的发现。首先，我们表明<strong>显著性对象分割是一个有效的问题，因为标注者之间具有高度一致性</strong>。其次，与凝视数据集不同，<strong>最广泛使用的显著性对象分割数据集存在严重的偏差</strong>。因此，当在更现实的图像上测试时，所有表现最好的显著性对象分割算法的泛化能力都很差。最后，我们展示了<strong>凝视点和显著对象之间存在强烈的关联</strong>。受这些发现的启发，在第4节中，我们<strong>提出了一种新的显著性对象分割模型</strong>。通过结合现有的基于凝视点的显著性模型和分割技术，我们的模型弥合了凝视点预测和显著性对象分割之间的差距。尽管其简单性，该模型在所有三个显著性对象数据集上明显优于最先进的显著性对象分割算法。</p>
<h2 id="Related-Works"><a href="#Related-Works" class="headerlink" title="Related Works"></a>Related Works</h2><p>在本节中，我们简要讨论了现有的凝视点预测和显著性对象分割模型。我们还探讨了显著性对象与通用对象分割（如CPMC [7, 19]）之间的关系。最后，我们回顾了与数据集偏差相关的研究。</p>
<h3 id="Fixation-prediction"><a href="#Fixation-prediction" class="headerlink" title="Fixation prediction"></a>Fixation prediction</h3><p>基于凝视点的自底向上显著性问题（fixation based bottom-up saliency）首先由[17]（1998）引入到计算机视觉领域。这类模型的目标是计算一个“显著性图”，以模拟人类的眼动行为。这些模型中通常使用基于补丁（patch-based） [17, 6, 14, 16, 26] 或像素（pixel-based） [15, 13] 的特征，随后进行局部或全局的交互步骤，以重新加权或重新归一化特征显著性值。为了定量评估不同凝视点算法的性能，通常使用 ROC 曲线下面积（AUC）来将显著性图与人类的眼动数据进行比较。最早的系统性凝视点预测数据集之一由[6]引入。在这篇论文中，Bruce 等人记录了 21 名受试者在 120 张自然图像上的眼动数据。在更近期的一篇论文中，[18] Judd 等人引入了一个更大规模的数据集，包括 1003 张图像和 15 名受试者。由于眼动追踪实验的特性，记录的凝视点位置误差在典型设置下可能高达 1°，或超过 30 个像素。因此，生成一个与人类数据完全匹配的像素级精确显著性图并不是必要的。事实上，正如[15]中所指出的，<strong>模糊化显著性图通常可以提高其 AUC 分数</strong>。</p>
<h3 id="Salient-object-segmentationf"><a href="#Salient-object-segmentationf" class="headerlink" title="Salient object segmentationf"></a>Salient object segmentationf</h3><p>直接使用凝视点预测算法生成的模糊显著性图并非易事。作为替代方案，Liu 等人 [21] （2007）提出了 MSRA-5000 数据集，其中对显著物体进行了<strong>边界框标注</strong>。基于“物体显著性”的理念，Achanta 等人 [1] （2009）进一步对 MSRA-5000 数据集中的 1000 张图像进行了<strong>像素级的物体轮廓遮罩标注</strong>。他们的论文表明，现有的凝视点算法如果使用 PR 曲线的 F-measure 进行基准测试，表现会很差。受这个新数据集的启发，一系列论文 [9, 23, 22] 提出了应对预测显著物体全分辨率遮罩的新挑战。Borji 等人 [4] （2012）最近的一篇综述中概述了显著物体算法的特征和性能。<strong>尽管凝视点预测和物体分割问题之间有着深层次的联系，但这两类主要计算模型之间却存在令人不安的隔离</strong>。显著物体分割算法发展出了一套与凝视点预测模型<strong>几乎没有重叠</strong>的技术。这主要是由于在标注和评估过程中存在一系列差异。典型的凝视点标注包含几个凝视点，而显著物体的标注通常包含一个或多个由数千个像素组成的正区域。不同的稀疏性先验显著限制了一类模型在另一类任务上的良好表现。</p>
<h3 id="Objectness-object-proposal-and-foreground-segments"><a href="#Objectness-object-proposal-and-foreground-segments" class="headerlink" title="Objectness, object proposal, and foreground segments"></a>Objectness, object proposal, and foreground segments</h3><p>在<strong>物体识别领域</strong>，研究人员感兴趣的是寻找与类别无关的物体 [10]。Alexe 等人 [2] 使用了低&#x2F;中层图像线索的组合（a combination of low&#x2F;mid level image cues）来衡量边界框的“物体性”。其他模型，如 CPMC [7, 19] 和 Object Proposal [11]，在不依赖类别特定信息的情况下生成候选物体的分割。然后对获得的“前景分割”或“物体建议”进行排名或评分，以粗略估计场景中的物体。上述文献中的<strong>评分&#x2F;排名功能与显著性概念有很多相似之处</strong>。事实上，[2] 将显著性图作为预测物体性的主要特征。在第4节中，我们提出了一个<strong>基于 CPMC 生成的前景分割的模型</strong>。这些方法与视觉显著性之间的一个根本区别是，物体检测器通常是穷尽的——它会寻找图像中的所有物体，而不考虑它们的显著性值。相比之下，显著物体检测器的目标是枚举超过某个显著性阈值的物体<strong>子集</strong>。正如我们将在第4节中讨论的那样，像 CPMC 这样的物体模型对其候选前景提案进行了排名。然而，排名靠前的（例如前200个）分割并不总是对应于显著物体或其部分。</p>
<h3 id="Datasets-and-dataset-bias"><a href="#Datasets-and-dataset-bias" class="headerlink" title="Datasets and dataset bias"></a>Datasets and dataset bias</h3><p>最近，研究人员开始定量分析数据集偏差及其在基准测试中产生的不利影响。数据集偏差来源于图像的选择 [25] 以及注释过程 [24]。在<strong>视觉显著性分析领域，最显著的偏差是中心偏差</strong>。中心偏差指的是被试者更频繁地注视屏幕中心的倾向 [24]。这种现象可能部分是由于在注视实验中被试者的头部被固定在下巴支架上，部分是由于摄影师偏好将物体对准照片的中心。研究表明，<strong>中心偏差对基准分数有显著影响</strong> [18, 26]。注视模型要么明确使用它 [18]，要么通过在显著性图的边界上填充来隐式使用它 [14, 16, 26]。为了公平地评估算法的真实预测能力，Tatler [24] 提出了一个“打乱的AUC”（shuffled-AUC, <strong>s-AUC</strong>）分数来规范中心偏差的影响。在 s-AUC 中，正样本来自测试图像的注视点，而负样本来自所有其他图像的注视点。</p>
<h2 id="Dataset-Analysis"><a href="#Dataset-Analysis" class="headerlink" title="Dataset Analysis"></a>Dataset Analysis</h2><p>在本文中，我们将在以下数据集上进行基准测试：Bruce [6]（2005）、Judd [18]（2009）、Cerf [8]（2008）、FT [1]（2009） 和 IS [20]（2013）。在这5个数据集中，Judd 和 Cerf 只提供注视点的真实标签。FT 只提供显著对象的真实标签。<strong>IS 同时提供注视点和显著对象的掩码</strong>。虽然 Bruce 数据集最初是为注视预测设计的，但<strong>最近</strong> [5] （2013）<strong>增加了70名受试者的数据</strong>，他们被指示在图像中标注出最显著的单个对象。在我们的对比实验中，我们包括了以下注视预测算法：ITTI [17]、AIM [6]、GBVS [1]、DVA [16]、SUN [26]、SIG [15]、AWS [13]；以及以下显著对象分割算法：FT [1]、GC [9]、SF [23] 和 PCAS [22]。这些算法是主要基准测试中的顶级表现者 [4]。<br><code>&lt;img src=&quot;https://vip.helloimg.com/i/2024/08/12/66b9ed4142c00.png&quot; width=&quot;70%&quot; /&gt;</code></p>
<blockquote>
<p>图1：PASCAL-S 数据集的示意图。我们的数据集提供了眼动注视点（图C）和显著对象（图D）的掩码。显著对象的标注基于完整的分割（图B）。PASCAL-S 与其前身的一个显著区别在于，PASCAL-S 中的每张图片均由多名标注者进行标注，且不限制显著对象的数量。</p>
</blockquote>
<h3 id="Psychophysical-experiments-on-the-PASCAL-S-dataset"><a href="#Psychophysical-experiments-on-the-PASCAL-S-dataset" class="headerlink" title="Psychophysical experiments on the PASCAL-S dataset"></a>Psychophysical experiments on the PASCAL-S dataset</h3><p>我们的 PASCAL-S 数据集基于 PASCAL VOC 2010 [12] 分割挑战赛的验证集构建。该子集包含 850 张自然图像。<strong>在眼动实验中，8 位受试者被要求执行“自由观看”任务，以探索图像</strong>。每张图像呈现 2 秒钟，并且在每观看 25 张图像后进行一次眼动追踪的重新校准。眼动数据使用 Eyelink 1000 眼动仪以 125Hz 的频率进行采样。</p>
<p><strong>在显著对象分割实验中，我们首先手动执行完整的分割</strong>，将图像中的所有对象裁剪出来。图1.B 显示了一个分割示例。当我们建立完整分割的真实标注时，我们遵循以下规则：1）我们不会故意标注图像的部分（例如人物的面部）；2）同一对象的断开区域分别标注；3）我们使用实心区域来近似表示中空对象，如自行车车轮。然后，我们进行了实验，<strong>邀请了 12 位受试者标注显著对象。给定一张图像，要求受试者通过点击选择显著对象</strong>。对此没有时间限制，也不限制受试者可以选择的对象数量。与我们的眼动实验类似，显著对象标注的指令故意保持模糊。<strong>每个分割部分的最终显著性值为其获得的点击总数除以参与标注的受试者人数</strong>。</p>
<h3 id="Evaluating-dataset-consistency"><a href="#Evaluating-dataset-consistency" class="headerlink" title="Evaluating dataset consistency"></a>Evaluating dataset consistency</h3><p>令人惊讶的是，许多当今广泛使用的显著对象分割数据集并没有确保标注者之间的一致性。为了比较我们 PASCAL-S 数据集与其他现有数据集中不同标注者之间的一致性水平，我们随机选择 50% 的受试者作为测试子集。然后，我们通过将其余受试者作为新的真实标签子集来对该测试子集的显著性图进行基准测试。</p>
<p>对于眼动任务，测试子集的显著性图是通过<strong>首先绘制所有来自测试子集的注视点，然后用 2D 高斯核（σ &#x3D; 0.05 图像宽度）过滤显著性图获得的</strong>。对于显著对象分割任务，测试&#x2F;真实标签显著性图是通过首先对测试&#x2F;真实标签子集的单个分割进行平均，然后以阈值 T h &#x3D; 0.53 生成每个子集的二值掩码来获得的。接着，我们计算测试子集的 AUC 分数或 F-measure，并用这个数值来表示标注者之间的一致性。</p>
<p>我们注意到，Bruce 数据集的分割图明显比 PASCAL-S 或 IS 数据集的分割图稀疏。Bruce 数据集中超过 30% 的分割图完全为空。这很可能是标注过程的结果。在 Borji 等人的实验 [5] 中，标注者被强制每张图片只选择一个对象。对于那些包含两个或更多同样显著对象的图像，在阈值处理后很可能会变为空白。<strong>虽然 Bruce 是为数不多的同时提供注视点和显著对象掩码的数据集之一，但它不适合我们的分析</strong>。</p>
<p>对于 PASCAL-S 和 IS 数据集，我们通过真实标签掩码对测试子集的分割图进行了 F-measure 基准测试。结果如表 1 所示。类似于我们对显著对象数据集的一致性分析，我们还评估了受试者之间眼动注视点的一致性（表 1）。尽管在复杂自然场景的背景下，“显著性”这一概念通常被认为定义不清，但我们<strong>在眼动注视和显著对象分割任务中观察到人类标注者之间的行为高度一致</strong>。<br><code>&lt;img src=&quot;https://vip.helloimg.com/i/2024/08/12/66b9f07299921.png&quot; width=&quot;50%&quot; /&gt;</code></p>
<h3 id="Benchmarking"><a href="#Benchmarking" class="headerlink" title="Benchmarking"></a>Benchmarking</h3><p>在本节中，我们对7个注视点算法进行了基准测试，包括 AWS [13]、AIM [6]、SIG [15]、DVA [16]、GBVS [14]、SUN [26] 和 ITTI [17]，测试的5个数据集包括 Bruce [6]、Cerf [8]、IS [20]、Judd [18] 以及我们的 PASCAL-S。对于显著对象分割，我们对4个算法进行了基准测试，包括 SF [23]、PCAS [22]、GC [9] 和 FT [1]，测试的4个数据集包括 FT [1]、IS [20] 以及我们的 PASCAL-S。对于所有算法，我们都使用了作者网站上的原始实现。此次分析的目的有两个：1）突出算法的泛化能力，2）探讨这些独立构建的数据集之间的差异。基准测试结果如图2所示。<br><code>&lt;img src=&quot;https://vip.helloimg.com/i/2024/08/12/66b9f257db3bc.png&quot; width=70% /&gt;</code></p>
<p>与注视点基准测试形成鲜明对比的是，当<strong>从流行的 FT 数据集迁移到其他数据集时，所有显著对象分割算法的性能都显著下降</strong>。所有4个算法的平均性能从 FT 数据集的 0.8341 下降到 IS 数据集的 0.5765（下降了 30.88%），在 PASCAL-S 数据集上进一步下降到 0.5530（下降了 33.70%）。这一结果令人警惕，因为从 FT 数据集到其他任何数据集的性能下降幅度，无论使用哪种算法，都轻松超过了显著对象分割算法在广泛使用的 FT 数据集上4年的进展。此外，算法之间的相对排名也会因数据集的不同而发生变化。</p>
<h3 id="Dataset-design-bias"><a href="#Dataset-design-bias" class="headerlink" title="Dataset design bias"></a>Dataset design bias</h3><p>数据集之间的性能差距清楚地表明了显著对象分割中存在新的挑战。然而，比起在另一个新数据集上开始基准测试竞赛，更重要的是<strong>找出性能下降的原因</strong>。在本节中，我们分析了以下图像统计数据，以发现当前<strong>显著对象分割数据集的相似性和差异性</strong>：</p>
<ol>
<li><strong>局部颜色对比度</strong>：分割或边界检测是大多数显著对象检测器中不可避免的一步。因此，检查边界是否“异常”容易分割是很重要的。为了估计边界的强度，我们在每个标注对象的边界位置裁剪一个5 × 5的图像块，分别计算前景和背景的RGB颜色直方图。然后，我们计算χ²距离来测量两个直方图之间的距离。值得注意的是，IS数据集的一些真实标注与对象边界并不完全对齐，导致局部对比度幅度的低估。</li>
<li><strong>全局颜色对比度</strong>：所谓的“显著性”也与前景和背景的全局对比度有关。类似于局部颜色对比度的测量，我们为每个对象计算其RGB直方图与背景RGB直方图之间的χ²距离。</li>
<li><strong>局部gPB边界强度</strong>：虽然颜色直方图距离捕捉了一些低级图像特征，但像gPB [3]这样的高级边界检测器结合了局部和全局线索，提供了更全面的边界存在估计。作为对我们局部颜色对比度测量的补充结果，我们对对象边界像素计算了3 × 3局部块的平均gPB响应。</li>
<li><strong>对象大小</strong>：在每张图像中，我们将对象的大小定义为图像中像素的比例。IS数据集中的大多数对象都非常小。</li>
</ol>
<p>如图3所示，FT数据集在局部&#x2F;全局颜色对比度以及gPB边界强度统计中表现突出。乍一看，我们观察到FT包含非自然强度的对象边界，这似乎是可以接受的，尤其是对于一个专注于显著对象分析的数据集而言。<strong>强边界与显著性的核心概念紧密相关</strong>：前景对象具有可辨识的边界，背景具有对比鲜明的颜色。事实上，FT数据集中的许多图像是展示显著性定义的经典示例。<strong>FT中的“显著性”概念比其他数据集的要明确得多</strong>。然而，在图像选择过程中减少显著性的模糊性，对于测试显著性来说是有害的，而不是有益的。图像选择过程和图像标注过程之间的混淆通过过度表达目标概念的理想属性，引入了一种特殊类型的偏差，并减少了负面示例的存在。我们称这种偏差为<strong>数据集设计偏差</strong>：在设计数据集时，<strong>图像标注过程应独立于图像选择过程</strong>。否则，数据集设计偏差将因正&#x2F;负示例的不成比例采样而出现。<br><code>&lt;img src=&quot;https://vip.helloimg.com/i/2024/08/12/66b9f37f3f6d3.png&quot; width=&quot;60%&quot; /&gt;</code></p>
<blockquote>
<p>图 3：显著对象分割数据集的图像统计信息。FT 数据集在局部&#x2F;全局颜色对比度和边界强度方面的统计数据与其他数据集不同。至于对象大小，PASCAL-S 数据集包含了大小对象的平衡混合。</p>
</blockquote>
<h3 id="Fixations-and-F-measure"><a href="#Fixations-and-F-measure" class="headerlink" title="Fixations and F-measure"></a>Fixations and F-measure</h3><p>之前的显著对象分割方法在F-measure上相比所有注视点算法报告了较大的优势。然而，这些比较大多是在FT数据集上进行的，而该数据集已被证明存在不可忽视的偏差。另一个导致注视点算法表现不佳的因素是中心偏差。主要的显著对象分割算法，如SF、PCAS和GC，已经对中心偏差进行了处理。相比之下，许多注视点预测算法，如AWS和SIG，并未实施中心偏差处理，因为它们期望通过s-AUC分数来消除中心偏差的影响。</p>
<p>为了减少中心偏差的影响，我们在所有注视点算法生成的显著性图上添加了一个固定的高斯（σ &#x3D; 图像宽度的0.4），然后在所有三个显著对象数据集上对这些算法进行基准测试。结果如图4所示。此外，我们还测试了IS和PASCAL-S数据集上的真实人类注视图的F-measure。每个注视图通过一个高斯核进行模糊处理（σ &#x3D; 图像宽度的0.03）。没有叠加中心偏差，因为人类的注视点已经严重偏向图像中心。</p>
<p>&lt;img src&#x3D;”<a target="_blank" rel="noopener" href="https://vip.helloimg.com/i/2024/08/12/66b9f4d275727.png">https://vip.helloimg.com/i/2024/08/12/66b9f4d275727.png</a>“ &gt;</p>
<blockquote>
<p>图 4：所有算法在 PASCAL-S 数据集上的 F-measure 结果。所有 CPMC+Fixation 的结果均使用前 K &#x3D; 20 的分割段获得。</p>
</blockquote>
<p><strong>当我们去除中心偏差和数据集设计偏差的影响后，注视点算法的性能变得非常具有竞争力</strong>。我们还注意到，真实人类注视图的F-measure相较于显著对象标注实验中的不同标注者一致性分数要低得多。这一性能差距可能是由于注视任务与显著对象标注任务之间的相关性较弱，或是<strong>注视点（点）与对象（区域）表示的不兼容性</strong>造成的。在下一节中，我们将展示后者更有可能是原因。一旦配备了适当的底层表示，人类注视图及其算法近似值将能够为显著对象分割生成准确的结果。</p>
<h2 id="From-Fixations-to-Salient-Object-Detection"><a href="#From-Fixations-to-Salient-Object-Detection" class="headerlink" title="From Fixations to Salient Object Detection"></a>From Fixations to Salient Object Detection</h2><p>如今，许多知名的显著对象算法包含以下两个组成部分：1）适合显著对象分割的表示方法，2）特征显著性的计算原则，例如区域对比度 [9] 或元素独特性 [23]。然而，这两个组成部分单独来看在计算机视觉领域并没有什么新意。一方面，检测对象的边界一直是分割算法自计算机视觉诞生以来的一个重要目标；另一方面，显著性的定义规则在注视分析中已被研究了几十年。</p>
<p>在本节中，我们通过<strong>结合现有的分割技术和基于注视的显著性</strong>，构建了一个显著对象分割模型。其核心思想是<strong>首先生成一组对象候选区域，然后利用注视算法对不同区域的显著性进行排名</strong>。这种简单的组合产生了一种新的显著对象分割方法，显著超越了所有之前的方法。</p>
<h3 id="Salient-object-object-proposal-and-fixations"><a href="#Salient-object-object-proposal-and-fixations" class="headerlink" title="Salient object, object proposal and fixations"></a>Salient object, object proposal and fixations</h3><p>我们的第一步是<strong>通过通用对象提议方法生成对象候选区域的分割</strong>。我们使用<strong>CPMC</strong> [7]来获得初始分割。CPMC是一种无监督框架，用于<strong>生成和排名</strong>可行的对象候选假设，而不需要类别特定的知识。该方法在图像上均匀初始化前景种子，并使用不同参数解决一系列最小割问题。输出的是一组作为重叠前景-背景分割的对象候选区域，以及它们的“对象性”分数。CPMC的目标是<strong>产生对潜在对象的过度覆盖</strong>，这些覆盖可以进一步用于诸如对象识别等任务。</p>
<p>CPMC类似对象提议的表示方式很容易适应显著对象分割。<strong>如果可以从对象候选池中找到所有显著对象，我们就可以将显著对象检测的问题简化为更容易的显著分割排序问题</strong>。对这些分割进行排序也简化了后处理步骤。由于这些分割已经保留了图像的边界，因此不需要明确的分割（例如图切 [9]）来获得最终的二进制对象掩膜。</p>
<p><strong>为了估计候选分割的显著性，我们利用了对象内注视点的空间分布</strong>。众所周知，<strong>注视点的密度直接反映了分割的显著性</strong>。对象上注视点的非均匀空间分布也提供了有用的线索来确定对象的显著性。例如，注视点位于分割中心将增加其被认为是对象的概率。为了保持我们的框架简单，我们在模型中不考虑类别特定或主体特定的注视模式。</p>
<h3 id="The-model"><a href="#The-model" class="headerlink" title="The model"></a>The model</h3><p>我们使用基于学习的框架来进行分割选择过程。这是通过<strong>为每个对象候选区域学习一个评分函数</strong>来实现的。<strong>给定一个提议的对象候选掩膜及其注视图，该函数估计该区域与真实值的重叠得分（交并比）</strong>，类似于 [19]。</p>
<p><strong>特征</strong>：我们提取两种类型的特征：<strong>形状特征</strong>和<strong>对象内的注视分布特征</strong>。形状特征描述分割的二进制掩膜，包括主轴长度、离心率、次轴长度和欧拉数。对于注视分布特征，我们首先对每个对象候选区域的主轴进行对齐，然后在对齐的对象掩膜上提取4 × 3的注视密度直方图。该直方图捕捉了对象内注视点的空间分布。一般来说，我们使用注视能量（fixation energy）一词来指代注视图中的像素值。对于人类注视，能量是当前位点的注视点数量的离散值；而对于算法生成的注视图，能量是一个连续值，范围从0到1，表示当前位点的注视概率。最终，为每个对象掩膜提取33维特征向量。特征的详细信息见表2。特别地，注视能量比定义为分割内注视能量之和除以整幅图像的注视能量之和。<br><code>&lt;img src=&quot;https://vip.helloimg.com/i/2024/08/12/66b9f6237ad28.png&quot; width=&quot;80%&quot;/&gt;</code></p>
<p><strong>学习</strong>：对于每个数据集，我们使用40%的图像进行随机抽样，<strong>训练一个包含30棵树的随机森林</strong>，其余图像用于测试。结果在训练集和测试集的10折随机拆分上进行平均。我们使用随机回归森林来<strong>预测对象掩膜的显著性分数</strong>。随机回归森林是决策树的集成。对于每个分支节点，从所有特征的随机子集中选择一个特征，并通过最小化均方误差（MSE）来设定决策边界。叶节点保持最终落在该节点上的所有训练样本的均值。最终结果是加权平均所有测试样本达到的叶节点的值。我们选择随机森林是因为我们的特征向量包含离散值（欧拉数），可以在决策树中轻松处理。</p>
<p>在测试阶段，每个分割是独立分类的。我们通过<strong>在像素级别平均前K个分割来生成显著对象分割</strong>，类似于 [7]。然后，我们<strong>使用简单的阈值化生成最终的对象掩膜</strong>。由于我们的显著性分数是针对图像分割定义的，这种简单策略能够产生准确的对象边界。值得注意的是，我们的方法中<strong>不使用外观特征</strong>，因为我们的目标是展示注视与显著对象分割之间的关系。我们的算法<strong>独立于底层的分割和注视预测算法</strong>，允许我们在不同的注视算法甚至人类注视之间进行切换。</p>
<h3 id="Limits-of-the-model"><a href="#Limits-of-the-model" class="headerlink" title="Limits of the model"></a>Limits of the model</h3><p>我们的模型包含两个独立的部分：一个<strong>提议区域的分割器</strong>和一个<strong>为每个区域赋予显著性分数的选择器</strong>。在本节中，我们通过逐一替换每个部分来探索模型的局限性。首先，我们量化选择器的性能上限，然后展示分割器可以达到的最佳结果。</p>
<p>为了<strong>测试选择器的上限</strong>，我们在PASCAL-S的数据集上使用人类注视图训练模型，并使用真实分割（例如图1.B）。在拥有完美分割器的情况下，该模型能够利用注视和形状信息准确估计分段的显著性。在测试集上，它达到了0.9201的F-measure，精确率P为0.9328，召回率R为0.7989。这个结果强有力地验证了我们的动机，即<strong>缩小注视与显著对象之间的差距</strong>。值得一提的是，该实验需要对整个数据集的所有对象进行完整分割。因此，PASCAL-S是唯一允许我们在理想分割器上测试选择器的数据集。</p>
<p>其次，我们<strong>测试CPMC分割算法的性能上限</strong>。我们将CPMC的每个分割与真实对象标注进行匹配，并贪婪地选择前200个分割中重叠得分最佳的分割。同样，结果非常积极。在前200个分割中，最佳的CPMC分割在PASCAL-S数据集上达到了0.8699的F-measure（P &#x3D; 0.8687，R &#x3D; 0.883）。在FT（F-measure &#x3D; 0.9496，P &#x3D; 0.9494，R &#x3D; 0.9517）和IS（F-measure &#x3D; 0.8416，P &#x3D; 0.8572，R &#x3D; 0.6982）数据集中也观察到了类似的结果。</p>
<h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><p>略</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>在本文中，我们探讨了注视预测与显著对象分割之间的联系，并<strong>提供了一个同时包含注视和显著对象标注的新数据集</strong>。我们在该数据集上对这两项任务进行了广泛的实验，并将结果与主要基准进行比较。我们的分析表明，显著对象的定义在人类受试者之间高度一致。同时，我们指出了主要显著对象基准中存在显著的数据集设计偏差。这种偏差主要是由于故意强调显著性概念所致。我们认为，显著对象分割的问题应该超越教科书中的视觉显著性示例。<strong>一个可能的新方向是研究注视与显著对象之间的强关联</strong>。</p>
<p>基于这一联系，我们提出了一种新的显著对象分割算法。我们的方法<strong>将问题解耦为分割生成过程，随后使用注视预测的显著性评分机制</strong>。这个简单的模型在所有主要数据集上均优于现有的最先进显著对象分割算法。我们的数据集与我们的方法为注视预测和显著对象分割这两个具有挑战性的问题提供了新的见解。</p>
<h2 id="结果展示"><a href="#结果展示" class="headerlink" title="结果展示"></a>结果展示</h2><img src="https://vip.helloimg.com/i/2024/08/12/66b9fc336035f.png" width="60%" />
> 图6：在FT数据集上显著对象分割结果的可视化。每两行比较一张图像的结果。第一行包括现有方法的结果（从左到右）：原始图像、真实掩膜、FT、GC、PCAS、SF和CPMC ranking；第二行显示我们的方法使用不同注视的结果（从左到右）：AIM、AWS、DVA、GBVS、ITTI、SIG和SUN。我们无法报告使用人类注视的结果。这些图像是通过对我们结果的F-measure进行降序排序后选择的。

<img src="https://vip.helloimg.com/i/2024/08/12/66b9fcf9933aa.png" />
> 图7：在IS数据集上显著对象分割结果的可视化。每两行比较一张图像的结果。第一行包括现有方法的结果（从左到右）：原始图像、真实掩膜、FT、GC、PCAS、SF和CPMC排名；第二行显示我们的方法使用不同注视的结果（从左到右）：人类注视、AIM、AWS、DVA、GBVS、ITTI、SIG和SUN。这些图像是通过对我们结果的F-measure进行降序排序后选择的。我们注意到，IS数据集更偏向于稀疏的显著性图，因为它包含相当一部分小的显著对象。

<img src="https://vip.helloimg.com/i/2024/08/12/66b9fd4868326.png"  />
> 图8：在PASCAL-S数据集上显著对象分割结果的可视化。每两行比较一张图像的结果。第一行包括现有方法的结果（从左到右）：原始图像、真实掩膜、FT、GC、PCAS、SF和CPMC排名；第二行显示我们的方法使用不同注视的结果（从左到右）：人类注视、AIM、AWS、DVA、GBVS、ITTI、SIG和SUN。这些图像是通过对我们结果的F-measure进行降序排序后选择的。

<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="F-measure"><a href="#F-measure" class="headerlink" title="F-measure"></a>F-measure</h3><p><strong>F-meausre</strong>（也称为F1-score）是一种常用的评估指标，特别是在分类问题中，用于衡量模型的精确度（Precision）和召回率（Recall）之间的平衡。它的定义如下：</p>
<p>F-measure 是精确率和召回率的调和平均数，其公式为：</p>
<p>$$<br>F1 &#x3D; 2 \times \frac{Precision \times Recall}{Precision + Recall}<br>$$</p>
<h3 id="AUC"><a href="#AUC" class="headerlink" title="AUC"></a>AUC</h3><p><strong>AUC</strong>（Area Under the Curve）通常指的是 ROC 曲线下的面积，广泛用于评估二分类模型的性能。它的定义和作用如下：</p>
<ol>
<li><strong>ROC曲线</strong>：ROC（Receiver Operating Characteristic）曲线是一种图形化的性能评估工具，它以假正例率（False Positive Rate, FPR）为横轴，真正例率（True Positive Rate, TPR，也称为召回率）为纵轴，绘制模型在不同阈值下的表现。</li>
<li><strong>AUC的计算</strong>：AUC是ROC曲线下方的面积，取值范围在0到1之间。AUC的值越大，表示模型的性能越好：<ol>
<li>AUC &#x3D; 0.5：模型的性能与随机猜测相当。</li>
<li>AUC &#x3D; 1：模型能够完美地将正类和负类分开。</li>
</ol>
</li>
</ol>
<h3 id="CPMC"><a href="#CPMC" class="headerlink" title="CPMC"></a>CPMC</h3><p>CPMC（Core Object Proposal with Multi-scale Context）是一种用于对象提议生成的无监督算法，其主要目标是从图像中提取出潜在的对象候选区域。以下是CPMC算法的简要介绍：</p>
<ol>
<li><strong>无监督学习</strong>：<ul>
<li>CPMC不依赖于特定类别的标注信息，而是通过无监督学习方法生成对象提议。这使得它能够在没有先验知识的情况下，适用于不同类型的对象。</li>
</ul>
</li>
<li><strong>区域提议</strong>：<ul>
<li>算法通过初始化前景种子点，结合图像的全局和局部信息，生成一组候选区域。这些候选区域被称为对象提议，算法力求覆盖图像中可能存在的所有对象。</li>
</ul>
</li>
<li><strong>多尺度处理</strong>：<ul>
<li>CPMC采用多尺度的方法来处理图像，从而能够捕捉到不同大小的对象。这有助于提高对大小各异的对象的检测能力。</li>
</ul>
</li>
<li><strong>最小割&#x2F;最大流算法</strong>：<ul>
<li>CPMC使用最小割&#x2F;最大流算法来优化对象的分割，通过解决一系列的最小割问题来得到更精确的对象边界。</li>
</ul>
</li>
<li><strong>“物体性”评分</strong>：<ul>
<li>CPMC为生成的每个对象提议计算“物体性”评分，表示该区域作为一个完整物体的可能性。这种评分有助于后续选择最具代表性的对象候选。</li>
</ul>
</li>
</ol>
<p>CPMC算法广泛应用于计算机视觉领域，尤其是在目标检测和图像分割等任务中。它可以作为其他算法的预处理步骤，提供潜在的对象区域，供后续分类或精细化处理。</p>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpg" alt="AwayX 微信支付">
        <p>微信支付</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>AwayX
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://awayx.online/2024/08/12/The%20Secrets%20of%20Salient%20Object%20Segmentation/" title="The Secrets of Salient Object Segmentation">http://awayx.online/2024/08/12/The Secrets of Salient Object Segmentation/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" rel="tag"><i class="fa fa-tag"></i> 论文笔记</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/08/09/Linear%20Algebra/" rel="prev" title="Linear Algebra">
      <i class="fa fa-chevron-left"></i> Linear Algebra
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/08/13/PASCAL%20VOC%20dataset/" rel="next" title="PASCAL-VOC 数据集介绍">
      PASCAL-VOC 数据集介绍 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC81OTUwMS8zNTk2Mw=="></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">2.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Related-Works"><span class="nav-number">3.</span> <span class="nav-text">Related Works</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dataset-Analysis"><span class="nav-number">4.</span> <span class="nav-text">Dataset Analysis</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#From-Fixations-to-Salient-Object-Detection"><span class="nav-number">5.</span> <span class="nav-text">From Fixations to Salient Object Detection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">6.</span> <span class="nav-text">Conclusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%93%E6%9E%9C%E5%B1%95%E7%A4%BA"><span class="nav-number">7.</span> <span class="nav-text">结果展示</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%99%84%E5%BD%95"><span class="nav-number">8.</span> <span class="nav-text">附录</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="AwayX"
      src="/images/TheRightToCry.jpg">
  <p class="site-author-name" itemprop="name">AwayX</p>
  <div class="site-description" itemprop="description">嘿嘿</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">32</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">42</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/awaygithub" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;awaygithub" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:away2557103310@outlook.com" title="E-Mail → mailto:away2557103310@outlook.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://yoursite.com/" title="http:&#x2F;&#x2F;yoursite.com" rel="noopener" target="_blank">Title</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2024-02 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">AwayX</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.4' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

</body>
</html>
