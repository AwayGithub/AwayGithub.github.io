<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/infinite-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/infinite-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"awayx.online","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"livere","storage":true,"lazyload":false,"nav":null,"activeClass":"livere"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="CVPR 2024 github：EricDengbowen&#x2F;QAGNet: Official repository for CVPR 2024 paper &#34;Advancing Saliency Ranking with Human Fixations: Dataset, Models and Benchmarks&#34;. (github.com)  Abstract显著性排序检测">
<meta property="og:type" content="article">
<meta property="og:title" content="Advancing Saliency Ranking with Human Fixations - Dataset, Models and Benchmarks">
<meta property="og:url" content="http://awayx.online/2024/08/18/Advancing%20Saliency%20Ranking%20with%20Human%20Fixations%20-%20Dataset,%20Models%20and%20Benchmarks/index.html">
<meta property="og:site_name" content="AwaySpace">
<meta property="og:description" content="CVPR 2024 github：EricDengbowen&#x2F;QAGNet: Official repository for CVPR 2024 paper &#34;Advancing Saliency Ranking with Human Fixations: Dataset, Models and Benchmarks&#34;. (github.com)  Abstract显著性排序检测">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://vip.helloimg.com/i/2024/08/18/66c1b6a0474ba.png">
<meta property="og:image" content="https://vip.helloimg.com/i/2024/08/18/66c1b69dbbab3.png">
<meta property="og:image" content="https://vip.helloimg.com/i/2024/08/18/66c1b699d720d.png">
<meta property="article:published_time" content="2024-08-18T09:00:50.000Z">
<meta property="article:modified_time" content="2024-08-18T12:51:21.530Z">
<meta property="article:author" content="AwayX">
<meta property="article:tag" content="论文笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://vip.helloimg.com/i/2024/08/18/66c1b6a0474ba.png">

<link rel="canonical" href="http://awayx.online/2024/08/18/Advancing%20Saliency%20Ranking%20with%20Human%20Fixations%20-%20Dataset,%20Models%20and%20Benchmarks/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Advancing Saliency Ranking with Human Fixations - Dataset, Models and Benchmarks | AwaySpace</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">AwaySpace</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">8</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">44</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">35</span></a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://awayx.online/2024/08/18/Advancing%20Saliency%20Ranking%20with%20Human%20Fixations%20-%20Dataset,%20Models%20and%20Benchmarks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/TheRightToCry.jpg">
      <meta itemprop="name" content="AwayX">
      <meta itemprop="description" content="嘿嘿">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AwaySpace">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Advancing Saliency Ranking with Human Fixations - Dataset, Models and Benchmarks
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2024-08-18 17:00:50 / 修改时间：20:51:21" itemprop="dateCreated datePublished" datetime="2024-08-18T17:00:50+08:00">2024-08-18</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文笔记</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Computer-Vision/" itemprop="url" rel="index"><span itemprop="name">Computer Vision</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Computer-Vision/Salient-Object-Ranking/" itemprop="url" rel="index"><span itemprop="name">Salient Object Ranking</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Computer-Vision/Salient-Object-Ranking/image/" itemprop="url" rel="index"><span itemprop="name">image</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Computer-Vision/Salient-Object-Ranking/image/2024/" itemprop="url" rel="index"><span itemprop="name">2024</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p><strong>CVPR 2024</strong></p>
<p><strong>github</strong>：<a target="_blank" rel="noopener" href="https://github.com/EricDengbowen/QAGNet">EricDengbowen/QAGNet: Official repository for CVPR 2024 paper &#34;Advancing Saliency Ranking with Human Fixations: Dataset, Models and Benchmarks&#34;. (github.com)</a></p>
</blockquote>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>显著性排序检测（Saliency Ranking Detection, SRD）已经成为计算机视觉中的一个挑战性任务，旨在不仅识别图像中的显著对象，还根据其显著性程度对其进行排序。现有的SRD数据集主要使用鼠标轨迹数据，这不足以捕捉人类视觉感知的复杂性。为了解决这个问题，本文介绍了<strong>第一个大规模的SRD数据集SIFR</strong>（Saliency Instance Fixation Ranking），该数据集使用<strong>真实的人类注视数据构建</strong>，从而更贴近真实的视觉感知过程。为了为该数据集建立基准，我们提出了<strong>QAGNet</strong>，这是一种新颖的模型，利用来自transformer检测器的显著性实例查询特征，在三层嵌套图中进行处理。通过大量实验，我们证明了我们的方法在两个广泛使用的SRD数据集以及我们新提出的数据集上超越了现有的最先进方法。代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/EricDengbowen/QAGNet">https://github.com/EricDengbowen/QAGNet</a> 上获取。</p>
<span id="more"></span>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p><img src="https://vip.helloimg.com/i/2024/08/18/66c1b6a0474ba.png" width="70%"/></p>
<blockquote>
<p>图 1. 我们提出的数据集、ASSR [30] 数据集和 IRSR [18] 数据集的对比，其中所有 SRD 的真实标签都经过着色处理。SOD 任务旨在检测像素级的显著对象，而不进行区分，而 SRD 任务则旨在为每个检测到的对象分配不同程度的显著性（列 (b) 和 (c)）。我们提出的数据集是基于眼动跟踪注视点精心制作的，而其他两个数据集则基于鼠标轨迹。</p>
</blockquote>
<p>显著目标检测 (Salient Object Detection, SOD) 旨在识别和分割图像中最具视觉显著性的物体。近年来，基于深度学习模型的研究[17, 19, 40, 41]在这一领域取得了显著成果。显著性排序检测 (Saliency Ranking Detection, SRD) 是一个新兴的任务，除了识别显著物体外，还根据显著性程度对其进行排序（图1(b)和(c)），旨在更好地反映人类在不同兴趣程度上关注多个物体的能力。这为图像字幕生成[36, 39]、图像裁剪[1]和自动驾驶[23]等下游任务提供了附加价值。</p>
<p>近期的 SRD 研究利用了 COCOSalRank [12]、ASSR [30] 和 IRSR [18] 数据集。这些数据集中的显著性排序是基于来自 SALICON [10] 数据集的鼠标轨迹计算得出的。这些鼠标轨迹是在人类观察者探索来自 MS-COCO [16] 数据集的模糊场景时捕获的。所有现有的 SRD 数据集<strong>都假设鼠标指向是眼动数据的合适替代</strong>，这些眼动数据可以在受控条件下通过视线测量捕获。然而，这一简化假设存在若干问题。首先，鼠标指向动作是自愿控制的，而大量的视线转移（快速眼动）是反射性的，因此<strong>会反映出显著性的不同方面</strong>[29]（图1(c)、(d)和(e)）。其次，鼠标指向动作和视线转移可能在人脑中以不同的参考框架处理。因此，鼠标指向行为也会反映出这两种响应模式之间转换的偏差和限制[3]。此外，当前的数据集还依赖于 MS-COCO 标注的准确性，这可能导致实例丢失或标注错误。</p>
<p>在本文中，我们<strong>提出了一个基于真实人类眼动追踪数据的大规模实例级 SRD 数据集</strong>，旨在训练和评估旨在捕捉和预测人类视觉注意力的模型。<strong>使用从 MS-COCO 中挑选的图像，专注于包含至少三个物体的场景，我们计算了多个用户在每个场景中的眼动注视时间</strong>。然后，我们系统地改进了所有图像的标注，添加了遗漏的物体，分离了相连的物体，并完善了现有的物体。与其他数据集不同，我们<strong>对给定场景中可能的显著实例数量不设上限</strong>，而是以注视次数作为指导，判断某一物体是否足够有趣以被包括在内。</p>
<p>我们还为 SRD 开发了一种强大的基准方法，并在现有数据集和我们新的数据集上对流行方法进行了评估。在显著性排序时，现有方法通常仅对检测器提出的高度显著的物体进行排序，而忽略了显著性较低的物体[6, 18, 30]，或者将输出限制在预定的最大数量[6, 9, 30, 31]。我们认为，显著性较低的物体仍然为显著性排序问题提供了有价值的信息，<strong>而选择固定数量的输出是一种简化，未能反映真实的人类视觉感知</strong>。我们的方法在基于查询的 transformer[2] 上构建了一个新颖的 GNN[28]，计算每个场景中大量物体的相对显著性排序，并在所有 SRD 数据集上提供了最先进的性能。</p>
<p>我们的主要贡献如下：</p>
<ul>
<li>我们提出了第一个基于人类自然观看模式的大规模相对显著性排序数据集。该数据集仅包含具有挑战性的多物体场景，并为所有显著物体提供了详细的实例级标注。</li>
<li>我们提供了一个在该数据集上的强大<strong>端到端基线方法，即 QAGNet</strong>。我们在一个新颖的三层嵌套 GNN 内部结构化 transformer 检测器的查询特征，以<strong>计算最多 200 个物体的相对显著性排序</strong>。</li>
<li>我们的实验结果展示了 QAGNet 中不同模块的有效性，并且我们的方法在两个广泛使用的数据集和我们新创建的数据集上，均显示出了显著优于其他竞争方法的性能。</li>
</ul>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><p>显著目标检测（Salient Object Detection, SOD）旨在突出场景中最具视觉吸引力或重要性的物体。类似于前景分割，SOD 的流行方法[17, 19, 24, 34, 35, 40, 41]和数据集[14, 33, 37, 38]并不区分不同的显著物体，而是对所有显著区域进行二值分割。近年来，显著性排序检测（Saliency Ranking Detection, SRD）这一扩展任务应运而生，旨在识别并对物体的显著性进行排序。该任务旨在更好地反映人类观察者在视觉场中感知不同物体相对重要性的能力。</p>
<p><img src="https://vip.helloimg.com/i/2024/08/18/66c1b69dbbab3.png" width="70%"/></p>
<blockquote>
<p>图 2. ASSR 和 IRSR 数据集中观察到的局限性。</p>
</blockquote>
<p>作为一个新兴任务，目前可供 SRD 使用的数据集数量有限。Islam 等人[9]将 Pascal-S [15] 显著目标检测数据集重新用于显著性排序任务的训练。然而，由于该数据集仅包含 850 张图像，其中许多（40.4%）仅包含一个显著物体，因此在 SRD 任务中使用有限。COCOSalRank [12] 以及广泛使用的 ASSR [30] 和 IRSR [18] 都结合了鼠标轨迹和 MS-COCO 多边形标注，将显著实例进行排序。COCO-SalRank 使用复杂的手工规则生成显著性排序，ASSR 利用人类注意力的转移，将先被注视的实例排序为更显著，而 IRSR 基于注视热图中的每个实例的最大值进行排序。在所有情况下，<strong>底层的注视图都是基于 SALICON 的鼠标轨迹信息生成的，这与真实的人类视线有本质上的不同</strong>。这些数据集<strong>还依赖于 MS-COCO 标注的准确性，而 MS-COCO 的标注并未将显著性作为下游任务来考虑</strong>。因此，场景中可能遗漏了关键显著物体的标注（如图 2(a) 和 (b) 中的广告牌和屏幕）。物体可能被错误地分组在一起并被分配相同的显著性排序（图 2(c)），或错误的物体被标注（图 2(d)）。在某些场景中（如图 2(e)），对显著物体数量施加任意限制可能会限制模型对人类感知的模拟能力。</p>
<p>目前已有多个方法基于上述数据集开发并训练了 SRD 模型。Islam 等人[9]将 SRD 建模为像素级的相对显著性问题，返回一个显著区域的二维图。SRD 更自然地被视为一个<strong>以物体为中心</strong>的问题，最近的方法也采取了这种以物体为中心的方法。Siris 等人[30]提出了一种利用自下而上和自上而下注意力推断注意力转移的模型。Liu 等人[18]采用改进的 Mask R-CNN [8] 和图推理[28]来解决 SRD 任务。Fang 等人[6]将物体的坐标嵌入到一个端到端框架中作为位置信息。Tian 等人[31]提出了一种双向物体-上下文方法，涉及空间注意力和物体级注意力。尽管已有一些进展，但大多数现有方法<strong>仅考虑了有限数量的潜在显著物体，或将输出限制在预定的最大数量</strong>。</p>
<h2 id="Proposed-Dataset"><a href="#Proposed-Dataset" class="headerlink" title="Proposed Dataset"></a>Proposed Dataset</h2><p>在本节中，我们介绍了我们称为显著性实例注视排序（SIFR）的新数据集。</p>
<h3 id="Image-Selection"><a href="#Image-Selection" class="headerlink" title="Image Selection"></a>Image Selection</h3><p>我们从 MS-COCO 数据集中挑选图像，该数据集包含各种类别的注释，覆盖多种场景。我们首先<strong>只选择包含三个或更多标注前景对象的图像</strong>，而不是像 ASSR 和 IRSR 中的两个。我们以这种方式限制数据集，以确保拥有足够多样化和复杂的视觉环境来测试显著性排序模型的性能。</p>
<h3 id="Gaze-Recording-and-Fixation-Filtering"><a href="#Gaze-Recording-and-Fixation-Filtering" class="headerlink" title="Gaze Recording and Fixation Filtering"></a>Gaze Recording and Fixation Filtering</h3><p>接下来，我们使用眼动跟踪系统进行“自由浏览”任务，观察<strong>八名参与者</strong>的观看习惯。凝视跟踪使用 Tobii Pro Nano（Tobii，瑞典）进行，采样频率设置为 60 Hz。每张图像按比例调整大小以适应全屏分辨率，并以固定的 3 秒时间呈现给受试者。为了保持眼动跟踪的准确性，每 200 张图像会执行一次重新校准程序。为缓解眼睛疲劳，参与者在每 200 张图像的间隔处可以选择休息或暂停凝视记录过程。需要注意的是，该过程并不快速，<strong>凝视记录的过程持续了六个月</strong>。在数据采集期间，我们在记录会话期间及会话之间监测注意力水平，以确保一致性。</p>
<p>然后，我们对每位参与者的原始凝视信息进行处理，以识别关注区域。我们使用基于速度的方法 [27] 将点分组为不同的注视事件，在这些事件中，观察者专注于某个对象 [25]。我们的方法会自动移除物体之间的快速眼动（扫视）。基于文献 [25] 中给出的注视平均时长，我们<strong>移除所有时长少于 200 毫秒的注视</strong>。此外，为减少中心偏见，我们<strong>移除了每位参与者首次捕获的注视点</strong>。经过过滤后，我们将每张图像对应的八位参与者的注视点进行汇总。</p>
<h3 id="Salient-Objects-Threshold-and-Annotation"><a href="#Salient-Objects-Threshold-and-Annotation" class="headerlink" title="Salient Objects Threshold and Annotation"></a>Salient Objects Threshold and Annotation</h3><p>我们旨在<strong>使用现有的 MS-COCO 注释、Mask-RCNN 和人工注释的最佳组合，识别并标注每个观察到的显著实例</strong>。在每个场景中，MS-COCO 将提供部分对象的注释，但这些对象可能并不显著，或者显著对象可能没有被注释。我们将根据需要移除、添加或细化注释，以确保所有观察到的显著对象都具有高质量的注释。首先，我们计算位于现有多边形内的注视点的平均数量 $m$ 及其对应的标准差 $\sigma$。然后，我们应用一个阈值 $N_{\text{fixation}} \geq m - \sigma$，其中 $N_{\text{fixation}}$ 表示多边形内的注视点数量。包含足够数量注视点的现有多边形将自动纳入数据集。我们使用 DBSCAN [4] 对其余的注视点进行聚类，如果这些聚类满足上述相同的阈值标准，我们将其标记为潜在的未注释对象。对于 MS-COCO 测试集中没有注释的图像，我们执行相同的过程，但使用 Mask R-CNN 的多边形初步估计值。</p>
<p><img src="https://vip.helloimg.com/i/2024/08/18/66c1b699d720d.png" width="70%" /></p>
<blockquote>
<p>图 4. 我们提出的数据集的示例。我们的注视图以代表阈值显著对象的点（红色）、在未注释对象上的注视聚类（绿色）和未被认为显著的注视聚类（黄色）为重点进行标注。</p>
</blockquote>
<p>ASSR 和 IRSR 数据集仅考虑 MS-COCO 中已正确注释的对象。相反，我们<strong>对任何注释质量差或缺失的分割进行重新注释</strong>。十名参与者参与了此标注任务，并按以下步骤进行：i）细化现有显著对象的边界，并为这些对象分配类别。由于该数据集专门用于 SRD 任务，我们利用 MS-COCO 中 80 个可用类别中的 12 个超类。ii）为任何未注释的对象创建实例级多边形。iii）确认是否有三个或更多实例级多边形保留，如果没有，则重新检查图像以决定是保留还是移除。显著对象<strong>根据每个实例中的注视点数量进行排序</strong>。根据 [18]，我们根据排名顺序均匀地为显著对象分配不同的显著性值。图 4 显示了我们数据集的三个示例。最终数据集包含 8389 张图像，分为 6701 张训练图像和 1688 张测试图像。</p>
<h3 id="Statistical-Analysis"><a href="#Statistical-Analysis" class="headerlink" title="Statistical Analysis"></a>Statistical Analysis</h3><h2 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h2><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Experimental-Setup"><a href="#Experimental-Setup" class="headerlink" title="Experimental Setup"></a>Experimental Setup</h3><p>数据集和指标：我们在两个公开可用的数据集 ASSR [30]、IRSR [18] 以及我们提出的数据集上进行了实验，并使用了 SRD 中的三种广泛使用的指标：显著对象排序（SOR）[9]、分割感知 SOR（SA-SOR）[18] 和平均绝对误差（MAE）。SOR 计算预测结果与真实值之间的 Spearman 秩序相关系数，这假设预测的实例与真实值相匹配，并且只考虑排名顺序。SA-SOR 利用交并比（IoU）来选择匹配的实例，然后计算 Spearman 秩序相关系数。我们按照 [18] 的方法将 IoU 阈值设置为 0.5。对于 MAE，我们直接计算生成的显著性排序图与真实值之间的像素级差异。</p>
<p>实现细节：我们在 QAG 层中利用图注意力网络 [32] 进行边缘计算和节点聚合，特征维度设置为 256，丢弃率为 0.2。按照 [2] 的方法，我们使用二元交叉熵损失和 dice 损失 [22] 作为掩码损失。对于排序预测，我们使用成对的 SRD 损失 [18]，并将边排序损失的权重设置为 3.0，最终排序损失的权重设置为 5.0。最终损失是掩码损失、显著性分类损失和排序损失的组合。在推理过程中，我们通过将显著性类别置信度与掩码置信度相乘来确定最终的置信度分数。仅保留置信度超过 0.7 的显著实例预测。我们使用 AdamW [21] 优化器对模型进行 30,000 次迭代训练，权重衰减设置为 1 × 10^-4。学习率从 2.5 × 10^-5 开始，并在第 22,000 次和 26,000 次迭代时减小 10 倍。我们将所有输入图像调整为 1024×1024，并且不应用其他预处理。我们使用 4 个 A6000 GPU，并将批量大小设置为 4，用于 Swin-L 主干网络。</p>
<h3 id="Comparisons-with-the-State-of-the-Art"><a href="#Comparisons-with-the-State-of-the-Art" class="headerlink" title="Comparisons with the State-of-the-Art"></a>Comparisons with the State-of-the-Art</h3><h3 id="Ablation-Studies"><a href="#Ablation-Studies" class="headerlink" title="Ablation Studies"></a>Ablation Studies</h3><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>在本文中，我们提出了首个基于人眼跟踪的、大规模实例级显著性排序数据集 SIFR。我们认为，相比于先前利用鼠标移动的数据，真实的人类注视点更能真实反映视觉注意力。此外，我们还提出了一种强大的基线方法 QAGNet，该方法利用了 Transformer 检测器中的查询特征，并将其融入了一种新颖的图架构中。我们的网络即使在复杂场景中也能成功生成高质量的显著性排序图。实验结果表明，我们的方法在两个现有数据集和我们的新数据集上均提供了强劲的性能表现。</p>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpg" alt="AwayX 微信支付">
        <p>微信支付</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>AwayX
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://awayx.online/2024/08/18/Advancing%20Saliency%20Ranking%20with%20Human%20Fixations%20-%20Dataset,%20Models%20and%20Benchmarks/" title="Advancing Saliency Ranking with Human Fixations - Dataset, Models and Benchmarks">http://awayx.online/2024/08/18/Advancing Saliency Ranking with Human Fixations - Dataset, Models and Benchmarks/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" rel="tag"><i class="fa fa-tag"></i> 论文笔记</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/08/18/HyperSOR%20-%20Context-Aware%20Graph%20Hypernetwork%20for%20Salient%20Object%20Ranking/" rel="prev" title="HyperSOR - Context-Aware Graph Hypernetwork for Salient Object Ranking">
      <i class="fa fa-chevron-left"></i> HyperSOR - Context-Aware Graph Hypernetwork for Salient Object Ranking
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/08/18/Domain%20Separation%20Graph%20Neural%20Networks%20for%20Saliency%20Object%20Ranking/" rel="next" title="Domain Separation Graph Neural Networks for Saliency Object Ranking">
      Domain Separation Graph Neural Networks for Saliency Object Ranking <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC81OTUwMS8zNTk2Mw=="></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">2.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Related-Work"><span class="nav-number">3.</span> <span class="nav-text">Related Work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Proposed-Dataset"><span class="nav-number">4.</span> <span class="nav-text">Proposed Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Proposed-Method"><span class="nav-number">5.</span> <span class="nav-text">Proposed Method</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Experiment"><span class="nav-number">6.</span> <span class="nav-text">Experiment</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">7.</span> <span class="nav-text">Conclusion</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="AwayX"
      src="/images/TheRightToCry.jpg">
  <p class="site-author-name" itemprop="name">AwayX</p>
  <div class="site-description" itemprop="description">嘿嘿</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">35</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/awaygithub" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;awaygithub" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:away2557103310@outlook.com" title="E-Mail → mailto:away2557103310@outlook.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://yoursite.com/" title="http:&#x2F;&#x2F;yoursite.com" rel="noopener" target="_blank">Title</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2024-02 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">AwayX</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.4' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

</body>
</html>
