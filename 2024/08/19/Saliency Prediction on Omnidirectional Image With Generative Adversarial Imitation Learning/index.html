<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/infinite-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/infinite-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"awayx.online","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"hide","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"livere","storage":true,"lazyload":false,"nav":null,"activeClass":"livere"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="TIP 2021 github：yanglixiaoshen&#x2F;SalGAIL: Saliency Prediction on Omnidirectional Image with Generative Adversarial Imitation Learning (TIP 2021) (github.com) MC2 论文  Abstract当观看全向图像（ODIs）时，观众可以通过移动头部来访">
<meta property="og:type" content="article">
<meta property="og:title" content="Saliency Prediction on Omnidirectional Image With Generative Adversarial Imitation Learning">
<meta property="og:url" content="http://awayx.online/2024/08/19/Saliency%20Prediction%20on%20Omnidirectional%20Image%20With%20Generative%20Adversarial%20Imitation%20Learning/index.html">
<meta property="og:site_name" content="AwaySpace">
<meta property="og:description" content="TIP 2021 github：yanglixiaoshen&#x2F;SalGAIL: Saliency Prediction on Omnidirectional Image with Generative Adversarial Imitation Learning (TIP 2021) (github.com) MC2 论文  Abstract当观看全向图像（ODIs）时，观众可以通过移动头部来访">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://vip.helloimg.com/i/2024/08/19/66c304649eb4a.png">
<meta property="og:image" content="https://vip.helloimg.com/i/2024/08/19/66c32393097d3.png">
<meta property="article:published_time" content="2024-08-19T11:01:25.000Z">
<meta property="article:modified_time" content="2024-08-19T11:02:49.075Z">
<meta property="article:author" content="AwayX">
<meta property="article:tag" content="论文笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://vip.helloimg.com/i/2024/08/19/66c304649eb4a.png">

<link rel="canonical" href="http://awayx.online/2024/08/19/Saliency%20Prediction%20on%20Omnidirectional%20Image%20With%20Generative%20Adversarial%20Imitation%20Learning/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Saliency Prediction on Omnidirectional Image With Generative Adversarial Imitation Learning | AwaySpace</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">AwaySpace</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">8</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">44</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">35</span></a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container"></div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="algolia-results">
  <div id="algolia-stats"></div>
  <div id="algolia-hits"></div>
  <div id="algolia-pagination" class="algolia-pagination"></div>
</div>

      
    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://awayx.online/2024/08/19/Saliency%20Prediction%20on%20Omnidirectional%20Image%20With%20Generative%20Adversarial%20Imitation%20Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/TheRightToCry.jpg">
      <meta itemprop="name" content="AwayX">
      <meta itemprop="description" content="嘿嘿">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AwaySpace">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Saliency Prediction on Omnidirectional Image With Generative Adversarial Imitation Learning
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2024-08-19 19:01:25 / 修改时间：19:02:49" itemprop="dateCreated datePublished" datetime="2024-08-19T19:01:25+08:00">2024-08-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文笔记</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Computer-Vision/" itemprop="url" rel="index"><span itemprop="name">Computer Vision</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Computer-Vision/Saliency-Prediction/" itemprop="url" rel="index"><span itemprop="name">Saliency Prediction</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Computer-Vision/Saliency-Prediction/ODI/" itemprop="url" rel="index"><span itemprop="name">ODI</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/Computer-Vision/Saliency-Prediction/ODI/2021/" itemprop="url" rel="index"><span itemprop="name">2021</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p><strong>TIP 2021</strong></p>
<p><strong>github</strong>：<a target="_blank" rel="noopener" href="https://github.com/yanglixiaoshen/SalGAIL">yanglixiaoshen/SalGAIL: Saliency Prediction on Omnidirectional Image with Generative Adversarial Imitation Learning (TIP 2021) (github.com)</a></p>
<p>MC2 论文</p>
</blockquote>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>当观看全向图像（ODIs）时，观众可以通过移动头部来访问不同的视口。因此，<strong>有必要预测观众在ODIs上的头部注视点</strong>。受到生成对抗模仿学习（GAIL）的启发，本文提出了一种新颖的方法来预测ODIs上头部注视的显著性，称为<strong>SalGAIL</strong>。首先，我们<strong>建立了一个关于ODIs的注意数据集</strong>（<strong>AOI</strong>）。与传统数据集不同，我们的AOI数据集规模较大，<strong>包含了30名观众观看600个ODIs时的头部（和眼动）注视数据</strong>。接下来，我们对AOI数据集进行了挖掘，发现了三个结论：（1）头部注视在不同观众之间具有一致性，并且这种一致性随着观众数量的增加而增长；（2）头部注视存在前中偏差（FCB）；（3）头部运动的幅度在各观众之间相似。根据这些发现，我们的SalGAIL方法应用深度强化学习（DRL）来预测一个观众的头部注视点，其中GAIL学习DRL的奖励，而不是传统的人为设计奖励。然后，开发了多流DRL以生成不同观众的头部注视点，并通过卷积预测的头部注视点生成ODI的显著性图。最后，实验验证了我们的方法在预测ODIs显著性图方面的有效性，显著优于11种最先进的方法。我们的AOI数据集和SalGAIL的代码可以在<a target="_blank" rel="noopener" href="https://github.com/yanglixiaoshen/SalGAIL">https://github.com/yanglixiaoshen/SalGAIL</a>上获得。</p>
<span id="more"></span>
<h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>近年来，随着虚拟现实（VR）的快速发展，全向图像（ODIs）变得越来越受欢迎。与传统的2D图像不同，全向图像提供了沉浸式和互动的VR观看体验。此外，全向图像能够提供球形刺激，这意味着观众可以通过头戴式显示器（HMD）访问360°×180°的视野。换句话说，人们可以自由移动头部，以改变视口以查看感兴趣的区域。因此，头部注视在建模全向图像上的视觉注意力中发挥了重要作用。因此，预测头部注视点是必要的，这在全向图像的许多应用中具有广泛的使用，例如压缩[1]、渲染[2]和视觉质量评估[3]。</p>
<p>最近，已经出现了几项研究[4]–[7]，用于预测全向图像上头部注视的显著性图。例如，Lebreton等人[4]开发了两个基于传统2D显著性预测模型的新模型用于全向图像上的头部注视显著性预测：布尔图基显著性模型（BMS）[8]和基于图的视觉显著性（GBVS）[9]。因此，这些模型分别被称为BMS360和GBVS360。Zhu等人[5]提出了一种多平面投影方法来预测全向场景上的头部注视，在该方法中生成了几个块来模拟视口。然后，在每个块中提取低级特征（空间频率、方向和颜色）和高级语义特征（汽车和人），这些特征进一步融合并映射到整体显著性图上。上述工作可以被视为基于手工特征的方法，因为它们用于预测头部注视的特征是手工设计的。事实上，深度学习的巨大成功推动了2D图像显著性预测的发展，这是与全向图像上的头部注视预测紧密相关的领域。然而，现有的方法中没有基于深度学习进行全向图像上头部注视的显著性预测。此外，基于手工特征的方法在整个全向图像（投影到2D平面上）上预测显著性图；但实际上，人类头部注视的转换仅依赖于观察到的视口。因此，整个全向图像的输入可能会导致预测准确性的降低。此外，由于全向图像从球面投影到平面（投影失真），基于手工特征的方法也会遭遇性能下降。因此，上述问题使得现有的基于手工特征的方法表现一般。</p>
<p>另一方面，<strong>现有的全向图像头部注视数据集都是小规模的</strong>，难以用于训练深度学习模型。具体来说，[10]是第一个包含人类注意力的全向图像数据集，其中包括63名观众在98个全向图像上的头部和眼动注视数据。此外，数据集[11]包含169名观众在22个全向图像上的头部和眼动注视数据。在[12]中，数据集包含40名观众观看的104个全向图像。然而，在[12]中只有头部注视数据，没有眼动注视数据。此外，数据集[13]包括27名观众在VR环境中观看的70个不同全向图像的注意力数据。</p>
<p>在本文中，我们<strong>建立了一个大规模的全向图像注意力数据集</strong>（称为AOI数据集），其中<strong>包含30名观众在600个全向图像上的头部和眼动注视数据</strong>。需要注意的是，我们的AOI数据集中的600个全向图像在分辨率和内容上都非常多样化。通过挖掘我们的数据集，我们发现观众在观看全向图像时头部注视具有高度的一致性。此外，我们还获得了一些额外的发现：（1）个体之间的头部注视分布存在差异；然而，当观众数量增加时，头部注视的一致性趋于增加和收敛；（2）头部注视具有前中偏差（FCB）特征；（3）我们AOI数据集中所有观众在所有全向图像上的头部运动幅度相似。基于上述发现，本文提出了一种基于生成对抗模仿学习（GAIL）的全向图像头部注视显著性预测方法，称为SalGAIL。</p>
<p>具体来说，我们的SalGAIL方法通过深度强化学习（DRL）模型来<strong>预测头部注视</strong>。在DRL模型中，我们将头部轨迹的方向视为DRL模型的动作，将观看的全向内容视为环境的观察。因此，DRL模型可以学习预测一个观众在全向图像上的头部注视点。然后，使用多流DRL生成不同观众的头部注视点，并通过卷积预测的头部注视点生成输入全向图像的显著性图。然而，与传统的DRL任务不同，我们的任务中的奖励难以获得和量化。相反，我们提出通过模仿训练阶段观众的头部轨迹来学习奖励。这种策略得益于最近GAIL的成功[25]，[26]。据我们所知，我们的方法是首个应用模仿DRL进行全向图像显著性预测的工作。模仿DRL旨在模仿人类观看全向图像的行为，因此“擅长”预测全向图像上头部注视的显著性。同时，我们的视口基础方法可以避免由于投影失真造成的性能下降，这是基于手工特征的方法所面临的问题。</p>
<p>总之，本文的主要贡献总结如下：</p>
<ul>
<li>我们建立了一个大规模的AOI数据集，并对全向图像上的人类注意力进行了若干发现。</li>
<li>我们提出了一种多流DRL模型来预测全向图像上的头部注视。</li>
<li>我们应用GAIL通过模仿观众的头部轨迹来学习我们DRL模型的奖励。</li>
</ul>
<h2 id="RELATED-WORKS"><a href="#RELATED-WORKS" class="headerlink" title="RELATED WORKS"></a>RELATED WORKS</h2><p>在本节中，我们回顾了用于全向图像（ODIs）显著性预测的方法和数据集。</p>
<h3 id="Saliency-Prediction-Approaches-for-ODIs"><a href="#Saliency-Prediction-Approaches-for-ODIs" class="headerlink" title="Saliency Prediction Approaches for ODIs"></a>Saliency Prediction Approaches for ODIs</h3><p>1) <strong>2D图像的显著性预测</strong>：在过去的二十年里，2D图像的显著性预测研究取得了广泛进展，如[8]、[27]–[32]所示。对于图像显著性预测任务，提出了许多有效的空间特征，这些特征采用自上而下或自下而上的策略进行人类注意力预测。具体而言，Itti等人[33]考虑了多尺度的低级特征，并将其结合以形成图像的显著性图。Harel等人[9]引入了基于图的视觉显著性（GBVS）模型，该模型定义了各种图像图上的马尔可夫链，并将图上位置的平衡分布视为激活和显著性值。考虑到自上而下的图像语义，Judd等人[28]提出了一种基于低级、中级和高级图像特征的显著性模型。此外，Borji等人[34]提出了将自下而上的低级特征与自上而下的认知视觉特征结合起来，然后从这些特征直接映射到眼动注视点的学习方法。受到深度学习的启发，深度神经网络（DNNs）已被成功应用于图像显著性预测，例如[35]–[42]。具体来说，Deepfix[39]中提出了一种基于DNN的结构，用于学习图像显著性的多尺度语义表示。此外，[38]中提出了显著性上下文（SALICON），通过有效的显著性相关损失函数对现有的DNN进行微调。在[42]中，提出了一种读取架构用于图像显著性预测，其中考虑了低级和DNN特征。</p>
<p>2) <strong>全向图像上眼动注视的显著性预测</strong>：尽管2D图像的显著性预测已经得到了很好的发展，但对于全向图像的显著性图预测<strong>只有少数几种方法</strong>。与2D图像不同，全向图像的显著性包括两种形式：头部注视和眼动注视。<strong>现有的大多数全向图像显著性预测方法集中在眼动注视上</strong>，包括[4]–[7]、[43]–[45]。特别地，Battisti等人[6]提出了一种用于预测全向图像眼动注视显著性图的模型，该模型基于低级和语义特征的结合。Startsev等人[43]提出了一种新颖的显著性预测方法，考虑了投影失真、赤道偏差和垂直边界效应，用于预测全向图像的眼动注视显著性。此外，Ling等人[7]考虑了人类的色彩感知，提出了一种基于颜色词典稀疏表示的模型，用于全向图像的显著性预测。此外，DNNs也被成功应用于全向图像的显著性预测[44]、[45]。SalNet360[44]被提出以微调传统的2D显著性预测CNN模型，以适应全向图像显著性预测任务。此外，SaltiNet[45]开发了一种DNN模型用于全向图像眼动注视预测，该模型基于时间感知的显著性信息表示，称为显著性体积。</p>
<p>3) <strong>全向图像上头部注视的显著性预测</strong>：特别是，<strong>针对全向图像头部注视显著性图的预测的相关工作相对较少</strong>[4]、[5]、[14]。Zhu等人[5]采用多视图投影方法生成全向图像上头部注视的显著性图。在他们的工作中，首先将全向图像投影到多视图块中以模拟视口。然后，提取所有块的自下而上和自上而下特征，并将这些特征融合以生成最终的头部注视显著性图。此外，[14]提出通过后处理方法将人类注意力的中心偏差添加到全向图像的显著性图中。Lebreton等人[4]开发了新的全向图像显著性预测模型，称为BMS360和GBVS360，这些模型基于传统的2D显著性预测模型BMS[8]和GBVS[9]。具体而言，BMS360应用了多融合显著性（FMS）[14]来去除边界约束。在GBVS360中，将等距矩形格式的输入全向图像投影到几个直线图像中，这些图像对应不同的视口，然后根据每个直线图像进行特征提取。最后，将结果特征图回投影到等距矩形域以生成显著性图。与全向图像相比，更多的工作[17]、[18]、[22]、[23]已经提出用于预测全向视频（ODVs）上的头部注视。例如，Xu等人[22]提出了一种深度强化学习（DRL）方法，用于全向视频上头部注视的显著性预测。此外，Zhang等人[18]提出了一种基于球面CNN的方案，用于全向视频的显著性预测。在下一节中，我们<strong>将概述现有的全向图像/全向视频注意力建模数据集</strong>。</p>
<h3 id="Attention-Datasets-for-ODIs"><a href="#Attention-Datasets-for-ODIs" class="headerlink" title="Attention Datasets for ODIs"></a>Attention Datasets for ODIs</h3><p><img src="https://vip.helloimg.com/i/2024/08/19/66c304649eb4a.png" /></p>
<p>为了学习全向图像（ODIs）/全向视频（ODVs）上的显著性模型，迫切需要包含头部注视和眼动注视的数据集。随着显著性预测方法的发展，最近已经建立了几个ODIs/ODVs数据集，以收集受试者在观看全向场景时的头部注视/眼动注视数据。表I总结了这些数据集的基本属性。据我们所知，Salient360（Rai等人[10]）和Saliency in VR（Sitzmann等人[11]）已在最近的ODI显著性预测工作中得到广泛应用。这些数据集的详细介绍如下：</p>
<p>1) <strong>Salient360</strong>：由Rai等人[10]提出，是最早用于显著性预测的ODI数据集之一。该数据集包含98个刺激图像，主要包括室内、室外和人物场景。对于每个ODI，至少有40名受试者在360°×180°的范围内自由移动头部观看刺激图像。这些刺激图像的最大分辨率为18,332×9,166。每个ODI展示25秒，并为所有受试者初始化相同的视口。然后，记录了眼动注视和头部注视的数据作为演示数据。最后，根据记录的演示数据，头部注视和眼动注视的显著性图都被转换为等距矩形格式。</p>
<p>2) <strong>Saliency in VR</strong>：由Sitzmann等人[11]提出，是另一个公共数据集，记录了1,980条头部注视和眼动注视轨迹，这些数据来自169名受试者观看22个静态ODIs。在他们的实验中，头部注视和眼动注视的数据在两种条件下使用HMD进行捕获：站立（称为VR站立）条件和坐着（称为VR坐着）条件。此外，[11]还收集了使用桌面显示器观察相同场景的数据，称为桌面条件。该数据集提供了使用三种不同的投影从球面到平面的头部注视和眼动注视的显著性图：即等距矩形、立方体图和基于补丁的投影。</p>
<p>这些注意力数据集可以有利于ODIs/ODVs的显著性预测方法。特别是，深度学习方法需要大规模数据来训练DNN模型。不幸的是，如表I所示，现有的数据集数据量不足，特别是对于ODIs。因此，我们建立了一个大规模的ODI显著性预测数据集，即AOI数据集。关于我们数据集的详细信息将在第III节中讨论。</p>
<h2 id="DATASET"><a href="#DATASET" class="headerlink" title="DATASET"></a>DATASET</h2><h3 id="Data-Collection"><a href="#Data-Collection" class="headerlink" title="Data Collection"></a>Data Collection</h3><p>1) <strong>刺激图像</strong>：首先，我们从Flickr [46] 收集了600个ODIs，其分辨率范围从4,000 × 2,000到24,028 × 12,014。每个ODI均以等距矩形格式和最大分辨率下载。请注意，所有600个ODIs均为创作共用版权下的内容。为了丰富数据集中的内容多样性，我们收集了四类ODIs，包括城市风光、自然风景、室内场景和人物场景。图2展示了我们数据集中每个类别的一些示例。</p>
<p><img src="https://vip.helloimg.com/i/2024/08/19/66c32393097d3.png" /></p>
<blockquote>
<p>图 2. 我们 AOI 数据集中的一些 ODI 示例。这些示例包括四个多样化的 ODI 类别：城市风光、自然风景、室内场景和人类场景。</p>
</blockquote>
<p>2) <strong>设备</strong>：我们通过HTC Vive和aGlass设备获取受试者的头部运动（HM）和眼动（EM）数据。其中，HTC Vive作为头戴显示器（HMD）用于观看ODIs。HM数据可以通过HTC Vive捕获，而aGlass设备则能够捕获视场（FoV）内的EM数据。请注意，aGlass设备嵌入在HTC Vive中。当受试者观看ODIs时，使用“虚拟桌面”显示所有图像，并应用[47]的软件记录HM和EM数据。整个HM数据和时间戳形成了观看ODI的头部轨迹，从中可以提取头部注视点。类似地，眼动注视点也可以从EM数据中获得。</p>
<p>3) <strong>受试者</strong>：共有30名受试者（19名女性和11名男性）参与了我们的实验，年龄范围为18至30岁，平均年龄为21岁。所有受试者均视力正常或经过矫正后正常。在观看ODIs之前，进行了简单的培训课程，以使受试者熟悉HTC Vive。此外，向所有受试者解释了实验程序。最后，受试者进行了观看ODIs的实验，具体程序如下。</p>
<p>4) <strong>实验过程</strong>：600个ODIs被随机且均等地分为两个组。每名受试者在不同的日期观看这两个ODI组，以避免疲劳。根据Salient360数据集[10]，观看每个ODI的时间设置为22秒。观看每张图像后，我们插入了一个灰色ODI，红点位于经度=0°和纬度=0°，受试者在注视红点后按下按钮进入下一个ODI。因此，当受试者观看下一个ODI时，他们的HM和EM可以重新初始化到相应等距矩形图像的中心。请注意，受试者在感到疲劳时可以休息。观看ODIs时，戴着HTC Vive的受试者被要求坐在舒适的旋转椅上，以便他们可以自由旋转360°。这样，图像中的所有全景区域都可以轻松访问。</p>
<p>5) <strong>原始数据</strong>：然后，原始HM数据以以下格式记录。对于EM数据，我们发现观看ODIs时<strong>HM和EM之间存在强相关性</strong>（详见我们的支持文档第1节）。因此，本文<strong>仅关注预测ODIs上的头部注视点</strong>。实际上，某个受试者观看一个ODI的HM数据可以用一个向量表示：[时间戳，HM俯仰角，HM偏航角]。具体而言，上述向量由时间戳和HM位置组成。 (1) <strong>时间戳</strong>：记录了两个相邻HM样本点之间的时间间隔，以毫秒为单位表示。 (2) <strong>HM位置</strong>：与HM位置相关的两个元素，包括2个欧拉角：俯仰角和偏航角。如图1所示，视口的位置（110°×110°）可以用纬度和经度表示，分别对应于俯仰角和偏航角。</p>
<p>6) <strong>HM和EM数据收集</strong>：HM数据通过HTC Vive的HMD捕获，而EM数据则通过aGlass DKI捕获，aGlass DKI是一个VR眼动追踪模块，嵌入在HTC Vive中。在我们的实验中，HM和EM数据在观看ODIs时被程序同时但分别捕获和记录。随后，捕获的HM和EM位置与时间步对齐，并传输到计算机进行存储。请注意，我们AOI数据集的所有数据均可在<a target="_blank" rel="noopener" href="https://github.com/yanglixiaoshen/SalGAIL">https://github.com/yanglixiaoshen/SalGAIL</a> 上在线获取。</p>
<h3 id="Data-Processing"><a href="#Data-Processing" class="headerlink" title="Data Processing"></a>Data Processing</h3><p>为了区分头部注视点和扫视，我们需要从原始头部运动（HM）数据中提取这些信息。尽管<strong>本文主要关注预测头部注视点的显著性图</strong>，但<strong>我们的数据集也可以用于未来工作的头部扫视预测</strong>。下面介绍了我们用于区分头部注视点和扫视的算法。<br>……</p>
<h2 id="SALGAIL-APPROACH"><a href="#SALGAIL-APPROACH" class="headerlink" title="SALGAIL APPROACH"></a>SALGAIL APPROACH</h2><h2 id="REWARD-LEARNING-FOR-SALGAIL-APPROACH"><a href="#REWARD-LEARNING-FOR-SALGAIL-APPROACH" class="headerlink" title="REWARD LEARNING FOR SALGAIL APPROACH"></a>REWARD LEARNING FOR SALGAIL APPROACH</h2><h2 id="EXPERIMENTAL-RESULTS"><a href="#EXPERIMENTAL-RESULTS" class="headerlink" title="EXPERIMENTAL RESULTS"></a>EXPERIMENTAL RESULTS</h2><h3 id="Settings"><a href="#Settings" class="headerlink" title="Settings"></a>Settings</h3><p>在这一节中，我们验证了所提出的 SalGAIL 方法的有效性。为此，我们将 AOI 数据集中的每一类别的全景图（ODI）<strong>随机分为训练集和测试集，比例为 5:1</strong>。因此，训练集包含 500 张 ODIs，测试集包含 100 张 ODIs。接下来，我们将 SalGAIL 方法的性能与其他先进的方法进行比较，这些方法包括 DVA [57]、BMS [8]、SALICON [38]、MLNet [35]、RAM [58]、BMS360 [4]、GBVS360 [4]、Startsev et al. [43]、Zhu et al. [5]、DHP [22] 和 Battisti et al. [6]。其中，DVA、BMS、SALICON、RAM 和 MLNet 是针对 2D 图像的最新显著性预测方法。其余方法则是近期用于预测 ODIs 上头部注视点显著性图的方法。只有 RAM、SALICON、MLNet 和 DHP 的训练模型可以在线获取，因此我们在我们的训练集上重新训练了这些模型，以确保公平比较。由于 RAM 是一种基于深度强化学习（DRL）的方法，重点是通过奖励优化图像分类准确性来预测注意力，因此我们重新设计了 RAM 中的奖励机制，以优化头部注视点的显著性预测。</p>
<p>在我们的 SalGAIL 方法中，生成器每个时间步的输入是上一个时间步预测的视口，该视口被投影到 2D 平面上并下采样到 84 × 84 像素。我们将 SalGAIL 方法中的流数 N 设置为 30，这相当于受试者的数量。在训练 SalGAIL 生成器时，我们调整了超参数以优化训练集上的累积折扣奖励。这些超参数的值列在表 II 中。表 II 还列出了鉴别器和策略选择器的关键超参数设置，这些超参数也是在训练 ODIs 上调整的。然后，使用 RMSprop 优化器 [59] 和 Adam 优化器 [60] 分别更新生成器和鉴别器/策略选择器的参数。所有实验均在配置为 Intel(R) Core(TM) i7-6700K CPU @ 4.0 GHz、32 GB RAM 和单个 Nvidia GeForce GTX 1080Ti GPU 的计算机上进行。</p>
<h3 id="Performance-Evaluation-on-SalGAIL"><a href="#Performance-Evaluation-on-SalGAIL" class="headerlink" title="Performance Evaluation on SalGAIL"></a>Performance Evaluation on SalGAIL</h3><h3 id="Generalization-Ability-Test"><a href="#Generalization-Ability-Test" class="headerlink" title="Generalization Ability Test"></a>Generalization Ability Test</h3><h3 id="Ablation-Analysis"><a href="#Ablation-Analysis" class="headerlink" title="Ablation Analysis"></a>Ablation Analysis</h3><h3 id="Potential-Application-of-SalGAIL"><a href="#Potential-Application-of-SalGAIL" class="headerlink" title="Potential Application of SalGAIL"></a>Potential Application of SalGAIL</h3><p>我们的 SalGAIL 方法在其他感知任务中也具有潜在应用，因为注意力机制已广泛应用于最先进的计算机视觉工作中 [61]，[62]。在此，我们展示了 SalGAIL 在 ODIs 中目标检测感知任务中的潜在应用。为此，我们在一个 <strong>ODI 目标检测数据集</strong> [63] 上进行了实验，其中注释的对象类别也在 COCO 数据集 [64] 中可用。然后，我们使用最先进的目标检测方法 YOLOv3 [65] 作为基线算法。基于 YOLOv3，我们通过将输入 ODI 与通过 SalGAIL 方法获得的显著性图相乘来过滤输入 ODI，称之为 YOLOv3+ attention。为了公平比较，YOLOv3 和 YOLOv3+ attention 都在数据集 [63] 的训练集上重新训练。最后，表 VI 通过平均精度 (AP) 和平均平均精度 (mAP) 这两个指标比较了这两种模型的目标检测结果。由于数据集 [64] 中仅提供了三类（即人、车和船）注释对象，我们计算了这三类对象的 AP 以进行评估。表 VI 的结果显示，YOLOv3+ attention 在这三类对象的 AP 以及 mAP 上都优于普通的 YOLOv3。这证实了<strong>通过嵌入我们 SalGAIL 方法的显著性图可以提升 ODI 中的目标检测性能</strong>。更重要的是，这表明我们的 SalGAIL 方法在其他 ODI 感知任务中的潜在应用。</p>
<h2 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h2><p>在本文中，我们提出了 SalGAIL 方法来预测 ODIs 上的头部注视显著性图。首先，我们建立了 AOI 数据集，该数据集包含了 30 位受试者在 600 个 ODI 上的头部注视和眼动注视数据。据我们所知，AOI 是目前最大的 ODI 注意力建模数据集。其次，我们对 AOI 数据集进行了挖掘，并获得了一些关于受试者在观看 ODIs 时头部注视的发现。第三，受到这些发现的启发，我们在 SalGAIL 方法中提出了一种多流深度强化学习 (DRL) 模型，用于预测 ODIs 上的显著性图，其中奖励通过模仿人类头部轨迹来学习。多 DRL 模型中，每个 DRL 流生成一个受试者的头部运动轨迹，然后从所有 DRL 流生成的头部运动轨迹中采样头部注视。实验也验证了我们的 SalGAIL 方法在不同数据集上的高生成能力。<strong>最后，我们提出了一种处理技术，通过高斯核对输入 ODI 的预测头部注视进行卷积，从而生成 ODI 的显著性图作为 SalGAIL 方法的输出</strong>。大量实验表明，我们的 SalGAIL 方法在预测 ODIs 上头部注视显著性图的性能上优于 11 种最先进的方法。对于未来研究方向，我们的 SalGAIL 方法可能用于去除一些 ODI 处理任务中的视觉冗余，例如 ODI 质量评估和压缩。这是未来工作的另一个有趣应用。</p>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpg" alt="AwayX 微信支付">
        <p>微信支付</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>AwayX
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://awayx.online/2024/08/19/Saliency%20Prediction%20on%20Omnidirectional%20Image%20With%20Generative%20Adversarial%20Imitation%20Learning/" title="Saliency Prediction on Omnidirectional Image With Generative Adversarial Imitation Learning">http://awayx.online/2024/08/19/Saliency Prediction on Omnidirectional Image With Generative Adversarial Imitation Learning/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

        

  <div class="followme">
    <p>欢迎关注我的其它发布渠道</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" rel="tag"><i class="fa fa-tag"></i> 论文笔记</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/08/19/Hierarchical%20Bayesian%20LSTM%20for%20Head%20Trajectory%20%20Prediction%20on%20Omnidirectional%20Images/" rel="prev" title="Hierarchical Bayesian LSTM for Head Trajectory Prediction on Omnidirectional Images">
      <i class="fa fa-chevron-left"></i> Hierarchical Bayesian LSTM for Head Trajectory Prediction on Omnidirectional Images
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/08/19/State-of-the-Art%20in%20360%C2%B0%20Video_Image%20Processing%20-%20Perception,%20Assessment%20and%20Compression/" rel="next" title="State-of-the-Art in 360° Video_Image Processing - Perception, Assessment and Compression">
      State-of-the-Art in 360° Video_Image Processing - Perception, Assessment and Compression <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC81OTUwMS8zNTk2Mw=="></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#INTRODUCTION"><span class="nav-number">2.</span> <span class="nav-text">INTRODUCTION</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RELATED-WORKS"><span class="nav-number">3.</span> <span class="nav-text">RELATED WORKS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DATASET"><span class="nav-number">4.</span> <span class="nav-text">DATASET</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SALGAIL-APPROACH"><span class="nav-number">5.</span> <span class="nav-text">SALGAIL APPROACH</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#REWARD-LEARNING-FOR-SALGAIL-APPROACH"><span class="nav-number">6.</span> <span class="nav-text">REWARD LEARNING FOR SALGAIL APPROACH</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#EXPERIMENTAL-RESULTS"><span class="nav-number">7.</span> <span class="nav-text">EXPERIMENTAL RESULTS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CONCLUSION"><span class="nav-number">8.</span> <span class="nav-text">CONCLUSION</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="AwayX"
      src="/images/TheRightToCry.jpg">
  <p class="site-author-name" itemprop="name">AwayX</p>
  <div class="site-description" itemprop="description">嘿嘿</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">35</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/awaygithub" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;awaygithub" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:away2557103310@outlook.com" title="E-Mail → mailto:away2557103310@outlook.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="fa fa-link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://yoursite.com/" title="http:&#x2F;&#x2F;yoursite.com" rel="noopener" target="_blank">Title</a>
        </li>
    </ul>
  </div>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2024-02 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">AwayX</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.4' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="//cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script>
<script src="//cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script>
<script src="/js/algolia-search.js"></script>














  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>

</body>
</html>
